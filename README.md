# Recent Advances about Bias in Vision and Language 
Maintained by [JINGJING JIANG](https://github.com/jingjing12110). Last update on 2020/11/18. 

## Table of Contents

- [Recent Advances about Bias in Vision and Language](#recent-advances-about-bias-in-vision-and-language)
  - [Table of Contents](#table-of-contents)
  - [Bias in Visual Question Answering](#bias-in-visual-question-answering)
  - [Bias in Image Captioning](#bias-in-image-captioning)
  - [Bias in Visual Grounding / Referring Expression Comprehension](#bias-in-visual-grounding--referring-expression-comprehension)
  - [Bias in Scene Graph Generation](#bias-in-scene-graph-generation)
  - [Bias in Other Vision-and-Language Task](#bias-in-other-vision-and-language-task)
- [Other Resources](#other-resources)
  - [Pretrained V + L Model / Representation Learning](#pretrained-v--l-model--representation-learning)


## Bias in Visual Question Answering

[Estimating semantic structure for the VQA answer space](https://arxiv.org/abs/2006.05726), arXiv 2006

[Counterfactual VQA: A Cause-Effect Look at Language Bias](https://arxiv.org/abs/2006.04315), arXiv 2006

[Roses are Red, Violets are Blue... But Should VQA expect Them To?](https://arxiv.org/abs/2006.05121), BMVC 2020, [[code](https://github.com/gqaood/GQA-OOD?utm_source=catalyzex.com)]

[Counterfactual Samples Synthesizing for Robust Visual Question Answering](https://arxiv.org/abs/2003.06576v1), CVPR 2020, [[code](https://github.com/yanxinzju/CSS-VQA?utm_source=catalyzex.com)]

[Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing](https://openaccess.thecvf.com/content_CVPR_2020/papers/Agarwal_Towards_Causal_VQA_Revealing_and_Reducing_Spurious_Correlations_by_Invariant_CVPR_2020_paper.pdf), CVPR 2020

[Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder](https://arxiv.org/abs/2007.06198), ECCV 2020

[Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles](https://arxiv.org/abs/2011.03856), EMNLP 2020

[MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering](https://arxiv.org/abs/2009.08566), EMNLP 2020, [[project](https://www.public.asu.edu/~tgokhale/)] 

[Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering](https://www.aclweb.org/anthology/2020.emnlp-main.265.pdf), EMNLP 2020

[Linguistically Driven Graph Capsule Network for Visual Question Reasoning](https://arxiv.org/abs/2003.10065), TPAMI 2020

[Overcoming Language Priors with Self-supervised Learning for Visual Question Answering](https://www.ijcai.org/Proceedings/2020/0151.pdf), IJCAI 2020, [[code](https://github.com/CrossmodalGroup/SSL-VQA)]

[A negative case analysis of visual grounding methods for VQA](https://arxiv.org/abs/2004.05704), ACL 2020, [[code](https://github.com/erobic/negative_analysis_of_grounding?utm_source=catalyzex.com)]

[Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies](https://arxiv.org/abs/2010.10802?context=cs.LG), NeurIPS 2020, [[code](https://github.com/itaigat/removing-bias-in-multi-modal-classifiers)]

[RUBi: Reducing unimodal biases for visual question answering](https://proceedings.neurips.cc/paper/2019/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf), NeurIPS 2019, [[code](https://github.com/cdancette/rubi.bootstrap.pytorch)]

[Quantifying and Alleviating the Language Prior Problem in Visual Question Answering](https://dl.acm.org/doi/abs/10.1145/3331184.3331186), SIGIR 2019, [[code](https://github.com/guoyang9/vqa-prior?utm_source=catalyzex.com)]

[Donâ€™t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases](https://arxiv.org/pdf/1909.03683.pdf), EMNLP 2019, [[code](https://github.com/chrisc36/debias?utm_source=catalyzex.com)]

[Answer Them All! Toward Universal Visual Question Answering Models](https://openaccess.thecvf.com/content_CVPR_2019/papers/Shrestha_Answer_Them_All_Toward_Universal_Visual_Question_Answering_Models_CVPR_2019_paper.pdf), CVPR 2019

[Explicit Bias Discovery in Visual Question Answering Models](https://openaccess.thecvf.com/content_CVPR_2019/papers/Manjunatha_Explicit_Bias_Discovery_in_Visual_Question_Answering_Models_CVPR_2019_paper.pdf), CVPR 2019

[Question-Conditioned Counterfactual Image Generation for VQA](https://arxiv.org/abs/1911.06352), CVPRW 2019

[Overcoming Language Priors in Visual Question Answering with Adversarial Regularization](https://arxiv.org/abs/1810.03649), NeurIPS 2018

[iVQA: Inverse Visual Question Answering](https://arxiv.org/abs/1710.03370), CVPR 2018 Spotlight

[Learning Answer Embeddings for Visual Question Answering](https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Learning_Answer_Embeddings_CVPR_2018_paper.pdf), CVPR 2018, [[code](https://github.com/hexiang-hu/answer_embedding)]

[Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering](https://arxiv.org/abs/1712.00377), CVPR 2018, [[code](https://github.com/AishwaryaAgrawal/GVQA?utm_source=catalyzex.com)]

[Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering](https://arxiv.org/abs/1612.00837), CVPR 2017, [[code](https://github.com/necla-ml/SNLI-VE?utm_source=catalyzex.com)], [[project](https://visualqa.org/)]

[Yin and yang: Balancing and answering binary visual questions](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Yin_and_Yang_CVPR_2016_paper.pdf), CVPR 2016

## Bias in Image Captioning

[Deconfounded image captioning: A causal retrospect](https://arxiv.org/abs/2003.03923), arXiv 2003

[Mitigating Gender Bias in Captioning Systems](https://arxiv.org/pdf/2006.08315.pdf), arXiv 2006, [[code](https://github.com/CaptionGenderBias2020/Mitigating_Gender_Bias_In_Captioning_System)]

[Off-Policy Self-Critical Training for Transformer in Visual Paragraph Generation](https://arxiv.org/abs/2006.11714),  arXiv 2006

[Understanding Image Captioning Models beyond Visualizing Attention](https://arxiv.org/abs/2001.01037), arXiv 2001

[Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation](https://arxiv.org/pdf/1911.11834.pdf), CVPR 2020, [[code](https://github.com/princetonvisualai/DomainBiasMitigation)]

[Improving Image Captioning with Better Use of Captions](https://arxiv.org/abs/2006.11807), ACL 2020, [[code](https://github.com/Gitsamshi/WeakVRD-Captioning?utm_source=catalyzex.com)]

[Learning to Collocate Neural Modules for Image Captioning](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.pdf), ICCV 2019, 

[Women Also Snowboard: Overcoming Bias in Captioning Models](https://openaccess.thecvf.com/content_ECCV_2018/papers/Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper.pdf), ECCV 2018, [[code](https://github.com/dtak/local-independence-public?utm_source=catalyzex.com)]

## Bias in Visual Grounding / Referring Expression Comprehension

[Visual Referring Expression Recognition: What Do Systems Actually Learn?](https://arxiv.org/abs/1805.11818), NAACL 2019, [[code](https://github.com/volkancirik/neural-sieves-refexp?utm_source=catalyzex.com)]

[CLEVR-Ref+: Diagnosing Visual Reasoning With Referring Expressions](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_CLEVR-Ref_Diagnosing_Visual_Reasoning_With_Referring_Expressions_CVPR_2019_paper.pdf), CVPR 2019, [[code](https://github.com/TheShadow29/awesome-grounding?utm_source=catalyzex.com)]

## Bias in Scene Graph Generation

[CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation](https://arxiv.org/abs/2009.07526), arXiv 2009

[Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models](https://arxiv.org/abs/2008.07832), arXiv 2008

[Unbiased Scene Graph Generation via Rich and Fair Semantic Extraction](https://arxiv.org/abs/2002.00176), arXiv 2002  

[PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph Generation](https://arxiv.org/abs/2009.00893), ACMMM 2020

[Unbiased Scene Graph Generation from Biased Training](https://arxiv.org/abs/2002.11949), CVPR 2020, [[code](https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch)]

[GPS-Net: Graph Property Sensing Network for Scene Graph Generation](https://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_GPS-Net_Graph_Property_Sensing_Network_for_Scene_Graph_Generation_CVPR_2020_paper.pdf), CVPR 2020, [[code](https://github.com/taksau/GPS-Net)]


## Bias in Other Vision-and-Language Task

[Counterfactual Vision and Language Learning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Abbasnejad_Counterfactual_Vision_and_Language_Learning_CVPR_2020_paper.pdf), CVPR 2020

[Diagnosing the Environment Bias in Vision-and-Language Navigation](https://arxiv.org/abs/2005.03086), IJCAI 2020, [[code](https://github.com/zhangybzbo/EnvBiasVLN?utm_source=catalyzex.com)]

[Adversarial Filters of Dataset Biases](https://arxiv.org/abs/2002.04108), ICML 2020


# Other Resources

## Pretrained V + L Model / Representation Learning
  
[Contrast and Classify: Alternate Training for Robust VQA](https://arxiv.org/abs/2010.06087), arXiv 2010, [[code](https://github.com/yashkant/concat-vqa)], [[project](https://yashkant.github.io/projects/concat-vqa.html)]

[DeVLBert: Learning Deconfounded Visio-Linguistic Representations](https://arxiv.org/abs/2008.06884), arXiv 2008, [[code](https://github.com/shengyuzhang/DeVLBert)]

[CVLP: Contrastive Visual Linguistic Pretraining](https://arxiv.org/abs/2007.13135), arXiv 2007, [[code](https://github.com/ArcherYunDong/CVLP-)]

[Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://arxiv.org/abs/2004.06165), ECCV 2020, [[code](https://github.com/microsoft/Oscar)]

[Large-Scale Adversarial Training for Vision-and-Language Representation Learning](https://arxiv.org/abs/2006.06195), NeurIPS 2020 Spotlight, [[code](https://github.com/zhegan27/VILLA)]

[12-in-1: Multi-Task Vision and Language Representation Learning](https://openaccess.thecvf.com/content_CVPR_2020/html/Lu_12-in-1_Multi-Task_Vision_and_Language_Representation_Learning_CVPR_2020_paper.html), CVPR 2020, [[code](https://github.com/facebookresearch/vilbert-multi-task)]

[UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/abs/1909.11740v3), ECCV 2020, [[code](https://github.com/ChenRocks/UNITER)]

[VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.08530v4), ICLR 2020, [[code](https://github.com/jackroos/VL-BERT)]

[Unified Vision-Language Pre-Training for Image Captioning and VQA](https://arxiv.org/abs/1909.11059v3), AAAI 2020, [[code](https://github.com/LuoweiZhou/VLP)]

[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/abs/1908.02265v1), NeurIPS 2019, [[code1](https://github.com/jiasenlu/vilbert_beta)], [[code2](https://github.com/facebookresearch/vilbert-multi-task)]

[VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557v1), arXiv 1908, [[code](https://github.com/uclanlp/visualbert)]

[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490v3), EMNLP 2019, [[code](https://github.com/airsplay/lxmert)]

## [Recent Advances in Vision and Language PreTrained Models (VL-PTMs)](https://github.com/jingjing12110/awesome-vision-language-pretraining-papers)

